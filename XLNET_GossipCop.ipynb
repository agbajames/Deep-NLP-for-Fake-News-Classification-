{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "up7mfbffUjVx",
    "outputId": "c9e6659e-71df-427c-8019-28f292fb1d06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/dataset/fakenewsnet\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.5719, Accuracy: 0.8478, Precision: 0.7953, Recall: 0.7683, F1 score: 0.7802\n",
      "Epoch: 2, Loss: 0.4247, Accuracy: 0.8439, Precision: 0.7843, Recall: 0.7874, F1 score: 0.7858\n",
      "Epoch: 3, Loss: 0.0362, Accuracy: 0.8455, Precision: 0.8088, Recall: 0.7331, F1 score: 0.7591\n",
      "Epoch: 4, Loss: 0.3088, Accuracy: 0.8469, Precision: 0.8287, Recall: 0.7173, F1 score: 0.7496\n",
      "Epoch: 5, Loss: 0.0085, Accuracy: 0.8471, Precision: 0.7973, Recall: 0.7594, F1 score: 0.7751\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set Working Directory\n",
    "%cd /content/drive/MyDrive/dataset/fakenewsnet/\n",
    "\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Text Processing \n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import regex\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load an pre-process the data\n",
    "\n",
    "df_gossipcop_fake = pd.read_csv(\"gossipcop_fake.csv\")\n",
    "df_gossipcop_real = pd.read_csv(\"gossipcop_real.csv\")\n",
    "df_politifact_fake = pd.read_csv(\"politifact_fake.csv\")\n",
    "df_politifact_real = pd.read_csv(\"politifact_real.csv\")\n",
    "\n",
    "# Generate labels True/Fake under new Target Column \n",
    "\n",
    "df_gossipcop_fake['label'] = ['Fake']*len(df_gossipcop_fake)\n",
    "df_gossipcop_real['label'] = ['Real']*len(df_gossipcop_real)\n",
    "df_politifact_fake['label'] = ['Fake']*len(df_politifact_fake)\n",
    "df_politifact_real['label'] = ['Real']*len(df_politifact_real)\n",
    "\n",
    "# Merge the four separate dataframes, by random mixing into a single dataframe called 'data'\n",
    "\n",
    "data = pd.concat([df_gossipcop_fake, df_gossipcop_real], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=0, Real=1)\n",
    "\n",
    "data['label'] = pd.get_dummies(data.label)['Real']\n",
    "\n",
    "# Delete columns not needed\n",
    "\n",
    "data = data.drop([\"id\", \"news_url\", \"tweet_ids\"], axis=1)\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace 'not' for 'n't'\n",
    "    text = re.sub(r\"(\\w+)n\\'t\", '\\g<1> not', text)\n",
    "\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove punctuation except '?'\n",
    "    text = re.sub(r'[^\\w\\s\\?]', '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\?]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words except 'not' and 'can'\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.discard('not')\n",
    "    stop_words.discard('can')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove trailing whitespaces\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "    # Identify the text column(s) to preprocess\n",
    "text_columns = ['title']\n",
    "\n",
    "# Preprocess each text column and store the results in new columns\n",
    "for column in text_columns:\n",
    "    # Create a new column name for the preprocessed text\n",
    "    new_column = column + '_preprocessed'\n",
    "\n",
    "# Apply the preprocessing function to the text column and store the results in the new column\n",
    "data[new_column] = data['title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data to a list of strings\n",
    "train_texts = train_data['title_preprocessed'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "val_texts = val_data['title_preprocessed'].tolist()\n",
    "val_labels = val_data['label'].tolist()\n",
    "\n",
    "\n",
    "# Load the XLNet tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "train_texts_tokens = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_texts_tokens = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create PyTorch DataLoader objects for your datasets\n",
    "train_dataset = TensorDataset(train_texts_tokens['input_ids'], train_texts_tokens['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(val_texts_tokens['input_ids'], val_texts_tokens['attention_mask'], val_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import XLNetForSequenceClassification, AdamW\n",
    "\n",
    "# Initialize the model\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "# Set up the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Evaluate the model\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        num_correct += (predictions == labels).sum().item()\n",
    "        num_total += labels.size(0)\n",
    "    \n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predictions.tolist())\n",
    "\n",
    "    accuracy = num_correct / num_total\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 score: {:.4f}'.format(epoch+1, loss.item(), accuracy, precision, recall, f1))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
