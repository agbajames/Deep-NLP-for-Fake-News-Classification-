{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "up7mfbffUjVx",
    "outputId": "c7d2a7dd-1c2a-461f-bd08-19a6b8ad97d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive/dataset/fakenewsnet\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.9/dist-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.9/dist-packages (from wordcloud) (1.22.4)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.9/dist-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from wordcloud) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (5.12.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->wordcloud) (4.39.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->wordcloud) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (0.1.98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6845, Accuracy: 0.5896, Precision: 0.7938, Recall: 0.5057, F1 score: 0.3814\n",
      "Epoch: 2, Loss: 0.6864, Accuracy: 0.7547, Precision: 0.7973, Recall: 0.7144, F1 score: 0.7185\n",
      "Epoch: 3, Loss: 0.1658, Accuracy: 0.8491, Precision: 0.8438, Recall: 0.8479, F1 score: 0.8455\n",
      "Epoch: 4, Loss: 0.0470, Accuracy: 0.8396, Precision: 0.8730, Recall: 0.8118, F1 score: 0.8235\n",
      "Epoch: 5, Loss: 0.0086, Accuracy: 0.8632, Precision: 0.8634, Recall: 0.8534, F1 score: 0.8573\n",
      "Epoch: 6, Loss: 0.0084, Accuracy: 0.8679, Precision: 0.8640, Recall: 0.8640, F1 score: 0.8640\n",
      "Epoch: 7, Loss: 0.1498, Accuracy: 0.8632, Precision: 0.8592, Recall: 0.8682, F1 score: 0.8613\n",
      "Epoch: 8, Loss: 0.1116, Accuracy: 0.8774, Precision: 0.8725, Recall: 0.8787, F1 score: 0.8749\n",
      "Epoch: 9, Loss: 0.0005, Accuracy: 0.8679, Precision: 0.8636, Recall: 0.8723, F1 score: 0.8659\n",
      "Epoch: 10, Loss: 0.0003, Accuracy: 0.8821, Precision: 0.8773, Recall: 0.8827, F1 score: 0.8795\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Set Working Directory\n",
    "%cd /content/drive/MyDrive/dataset/fakenewsnet/\n",
    "\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# Text Processing \n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import regex\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load an pre-process the data\n",
    "\n",
    "df_gossipcop_fake = pd.read_csv(\"gossipcop_fake.csv\")\n",
    "df_gossipcop_real = pd.read_csv(\"gossipcop_real.csv\")\n",
    "df_politifact_fake = pd.read_csv(\"politifact_fake.csv\")\n",
    "df_politifact_real = pd.read_csv(\"politifact_real.csv\")\n",
    "\n",
    "# Generate labels True/Fake under new Target Column \n",
    "\n",
    "df_gossipcop_fake['label'] = ['Fake']*len(df_gossipcop_fake)\n",
    "df_gossipcop_real['label'] = ['Real']*len(df_gossipcop_real)\n",
    "df_politifact_fake['label'] = ['Fake']*len(df_politifact_fake)\n",
    "df_politifact_real['label'] = ['Real']*len(df_politifact_real)\n",
    "\n",
    "# Merge the four separate dataframes, by random mixing into a single dataframe called 'data'\n",
    "\n",
    "data = pd.concat([df_politifact_fake, df_politifact_real], axis=0).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Target column is made of string values True/Fake, let's change it to numbers 0/1 (Fake=0, Real=1)\n",
    "\n",
    "data['label'] = pd.get_dummies(data.label)['Real']\n",
    "\n",
    "# Delete columns not needed\n",
    "\n",
    "data = data.drop([\"id\", \"news_url\", \"tweet_ids\"], axis=1)\n",
    "\n",
    "# Define the preprocessing function\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace 'not' for 'n't'\n",
    "    text = re.sub(r\"(\\w+)n\\'t\", '\\g<1> not', text)\n",
    "\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    # Remove punctuation except '?'\n",
    "    text = re.sub(r'[^\\w\\s\\?]', '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s\\?]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words except 'not' and 'can'\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.discard('not')\n",
    "    stop_words.discard('can')\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Remove trailing whitespaces\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "    # Identify the text column(s) to preprocess\n",
    "text_columns = ['title']\n",
    "\n",
    "# Preprocess each text column and store the results in new columns\n",
    "for column in text_columns:\n",
    "    # Create a new column name for the preprocessed text\n",
    "    new_column = column + '_preprocessed'\n",
    "\n",
    "# Apply the preprocessing function to the text column and store the results in the new column\n",
    "data[new_column] = data['title'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import XLNetTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data to a list of strings\n",
    "train_texts = train_data['title_preprocessed'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "val_texts = val_data['title_preprocessed'].tolist()\n",
    "val_labels = val_data['label'].tolist()\n",
    "\n",
    "\n",
    "# Load the XLNet tokenizer\n",
    "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "\n",
    "# Convert your data to PyTorch tensors\n",
    "train_texts_tokens = tokenizer(train_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_texts_tokens = tokenizer(val_texts, truncation=True, padding=True, max_length=128, return_tensors='pt')\n",
    "val_labels = torch.tensor(val_labels)\n",
    "\n",
    "# Create PyTorch DataLoader objects for your datasets\n",
    "train_dataset = TensorDataset(train_texts_tokens['input_ids'], train_texts_tokens['attention_mask'], train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TensorDataset(val_texts_tokens['input_ids'], val_texts_tokens['attention_mask'], val_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "from transformers import XLNetForSequenceClassification, AdamW\n",
    "\n",
    "# Initialize the model\n",
    "model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=2)\n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Set up the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Set up the device (CPU or GPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # # Evaluate the model\n",
    "    # num_correct = 0\n",
    "    # num_total = 0\n",
    "    # for batch in test_loader:\n",
    "    #     input_ids = batch[0].to(device)\n",
    "    #     attention_mask = batch[1].to(device)\n",
    "    #     labels = batch[2].to(device)\n",
    "\n",
    "    #     outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    #     logits = outputs.logits\n",
    "    #     predictions = torch.argmax(logits, dim=-1)\n",
    "    #     num_correct += (predictions == labels).sum().item()\n",
    "    #     num_total += labels.size(0)\n",
    "\n",
    "    # accuracy = num_correct / num_total\n",
    "    # print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch+1, loss.item(), accuracy))\n",
    "\n",
    "    # Evaluate the model\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "    num_correct = 0\n",
    "    num_total = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        num_correct += (predictions == labels).sum().item()\n",
    "        num_total += labels.size(0)\n",
    "    \n",
    "        y_true.extend(labels.tolist())\n",
    "        y_pred.extend(predictions.tolist())\n",
    "\n",
    "    accuracy = num_correct / num_total\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1 score: {:.4f}'.format(epoch+1, loss.item(), accuracy, precision, recall, f1))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
